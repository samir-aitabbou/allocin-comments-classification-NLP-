{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1620a3f3",
   "metadata": {},
   "source": [
    "# in this second version of LSTM I will \n",
    "\n",
    "      1° balance train and dev data \n",
    "      2° give equal weights to all target classes\n",
    "      3° increase training  epochs\n",
    "      4° decrease training  batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7638e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Initialisation du générateur de nombres aléatoires\n",
    "random.seed(123)\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r'''\\w'|\\w+|[^\\w\\s]''')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Reshape, SpatialDropout1D\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.utils import class_weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bfb623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Télécharge les données nécessaires pour le tokenizer (segmenteur) de phrases et de mots\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Télécharge la liste de mots vides (stopwords) pour différentes langues\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c0fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('cleaned_data/train_cleaned.csv',index_col=0)\n",
    "dev_data   = pd.read_csv('cleaned_data/dev_cleaned.csv',index_col=0)\n",
    "test_data  =  pd.read_csv('cleaned_data/test_cleaned.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f399ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train_data shape\",train_data.shape)\n",
    "print(\"dev_data shape\",dev_data.shape)\n",
    "print(\"test_data shape\",test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beec910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET ONLY 1/4 OF TRAIN AND DEV DATA DUE TO COMPUTATION RESOURCES LIMITATIONS\n",
    "# train_data = train_data.head(200)\n",
    "# dev_data = dev_data.head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae2b06",
   "metadata": {},
   "source": [
    "# Balancing label classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e438c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['note'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed2d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data['note'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d47bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the same minority classe size in all other classes in train and dev data \n",
    "\n",
    "from sklearn.utils import resample, class_weight\n",
    "\n",
    "\n",
    "# Calculez la taille de la classe minoritaire\n",
    "train_minority_class_size = min(train_data['note'].value_counts())\n",
    "dev_minority_class_size = min(dev_data['note'].value_counts())\n",
    "\n",
    "# Sous-échantillonnez les classes majoritaires pour avoir la même taille que la classe minoritaire\n",
    "train_data = pd.concat([\n",
    "    resample(train_data[train_data['note'] == note], replace=True, n_samples=train_minority_class_size)\n",
    "    for note in train_data['note'].unique()\n",
    "])\n",
    "\n",
    "dev_data = pd.concat([\n",
    "    resample(dev_data[dev_data['note'] == note], replace=True, n_samples=dev_minority_class_size)\n",
    "    for note in dev_data['note'].unique()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6141c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['note'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d32b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data['note'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42951849",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['commentaire'] = train_data['commentaire'].astype(str)\n",
    "dev_data['commentaire']   = dev_data['commentaire'].astype(str)\n",
    "test_data['commentaire']  = test_data['commentaire'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d60adf",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2cbae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['commentaire']\n",
    "X_dev = dev_data['commentaire']\n",
    "X_test = test_data['commentaire']\n",
    "\n",
    "y_train = train_data['note']\n",
    "y_dev = dev_data['note']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df59ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c3864",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bf433b",
   "metadata": {},
   "source": [
    "# Y one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aaa251",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hotencoding = to_categorical(y_train, num_classes=10)\n",
    "print(y_train_one_hotencoding[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4265f7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev_one_hotencoding = to_categorical(y_dev, num_classes=10)\n",
    "print(y_dev_one_hotencoding[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9cbf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(X_train.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca6a5f3",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1088478",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_most_common_words = 10000\n",
    "max_len = 180\n",
    "\n",
    "tokenizer = Tokenizer(num_words=n_most_common_words)\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train) \n",
    "X_dev = tokenizer.texts_to_sequences(X_dev)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_dev = pad_sequences(X_dev, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d53bba",
   "metadata": {},
   "source": [
    "# Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29470dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Sauvegarder le tokenizer\n",
    "with open('Lstm_tokenizer.pkl', 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c25da79",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2dde14",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs  = 20\n",
    "emb_dim = 128\n",
    "batch_size = 32\n",
    "# batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7275537",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((X_train.shape, y_train_one_hotencoding.shape, X_dev.shape, y_dev_one_hotencoding.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24224d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_most_common_words, emb_dim, input_length=X_train.shape[1]))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=False))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "# IMPROVEMENT\n",
    "# give equal class weights\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "# Convert class weights to a dictionary for use with Keras\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Create callbacks\n",
    "filepath = 'Lstm_best_model_improved.hdf5'\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=filepath,\n",
    "    monitor='val_accuracy',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=3,  # Increased patience\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, early_stopping]\n",
    "\n",
    "# Training\n",
    "start_time = time.time()\n",
    "history = model.fit(X_train, y_train_one_hotencoding, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_dev, y_dev_one_hotencoding), verbose=1, callbacks=callbacks, class_weight=class_weight_dict)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(execution_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0314e9",
   "metadata": {},
   "source": [
    "# save train and validation accuracy/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83587341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_plot.png')  # Save the plot as an image\n",
    "\n",
    "# Clear the figure for the next plot\n",
    "plt.figure()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.savefig('loss_plot.png')  # Save the plot as an image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd8ab38",
   "metadata": {},
   "source": [
    "# Test and saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea05a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Charger le tokenizer depuis le fichier\n",
    "# with open('Lstm_tokenizer.pkl', 'rb') as tokenizer_file:\n",
    "#     loaded_tokenizer = pickle.load(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145ffd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # model = load_model('MLP_Best_One.hdf5'),this command doesn't work for me, i had to change the saved model path\n",
    "\n",
    "# # modelpath = \"C:/trained_Models/Lstm_best_model_improved.hdf5\"\n",
    "\n",
    "\n",
    "# # Load the model with compile=False\n",
    "# Lstm_best_model = load_model(modelpath)\n",
    "\n",
    "# X_test = test_data['commentaire']\n",
    "# # X_test = tokenizer.texts_to_sequences(X_test)\n",
    "# X_test = loaded_tokenizer.texts_to_sequences(X_dev)\n",
    "# X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "# predictions = Lstm_best_model.predict(X_test)\n",
    "# argmax_predictions = np.argmax(predictions,axis =1)\n",
    "\n",
    "# print(\"argmax_predictions: \",argmax_predictions)\n",
    " \n",
    "# # # generate the plateform test data format        \n",
    "# with open(\"LSTM_ID_Prediction_improved.txt\", \"w\") as f:\n",
    "#     for i in range(len(test_data['review_id'])):\n",
    "#         prediction = (argmax_predictions[i] + 1) / 2\n",
    "#         line = f\"{test_data['review_id'].iloc[i]} {str(prediction).replace('.', ',')}\\n\"\n",
    "#         f.write(line)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8227583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8397bae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170fe3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
