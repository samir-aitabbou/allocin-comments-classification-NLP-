{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad22817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "# Initialisation du générateur de nombres aléatoires\n",
    "random.seed(123)\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r'''\\w'|\\w+|[^\\w\\s]''')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense,Dropout,Embedding,LSTM,BatchNormalization\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b9a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Télécharge les données nécessaires pour le tokenizer (segmenteur) de phrases et de mots\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Télécharge la liste de mots vides (stopwords) pour différentes langues\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36055978",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('cleaned_data/train_cleaned.csv',index_col=0)\n",
    "dev_data   = pd.read_csv('cleaned_data/dev_cleaned.csv',index_col=0)\n",
    "test_data  =  pd.read_csv('cleaned_data/test_cleaned.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22624c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET ONLY 1/4 OF TRAIN AND DEV DATA DUE TO COMPUTATION RESOURCES LIMITATIONS\n",
    "# train_data = train_data.head(200)\n",
    "# dev_data = dev_data.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec65c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['note'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5dd91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data['note'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0441b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the same minority classe size in all other classes in train and dev data \n",
    "\n",
    "from sklearn.utils import resample, class_weight\n",
    "\n",
    "\n",
    "# Calculez la taille de la classe minoritaire\n",
    "train_minority_class_size = min(train_data['note'].value_counts())\n",
    "dev_minority_class_size = min(dev_data['note'].value_counts())\n",
    "\n",
    "# Sous-échantillonnez les classes majoritaires pour avoir la même taille que la classe minoritaire\n",
    "train_data = pd.concat([\n",
    "    resample(train_data[train_data['note'] == note], replace=True, n_samples=train_minority_class_size)\n",
    "    for note in train_data['note'].unique()\n",
    "])\n",
    "\n",
    "dev_data = pd.concat([\n",
    "    resample(dev_data[dev_data['note'] == note], replace=True, n_samples=dev_minority_class_size)\n",
    "    for note in dev_data['note'].unique()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb974ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['note'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef82db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data['note'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e06416",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['commentaire'] = train_data['commentaire'].astype(str)\n",
    "dev_data['commentaire'] = dev_data['commentaire'].astype(str)\n",
    "test_data['commentaire'] = test_data['commentaire'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98372c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments_list = train_data['commentaire'].values.tolist()\n",
    "dev_comments_list  = dev_data['commentaire'].values.tolist()\n",
    "test_comments_list  = test_data['commentaire'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d45220",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_all_comments = train_comments_list + dev_comments_list + test_comments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8cd823",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_all_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6709e97b",
   "metadata": {},
   "source": [
    "# Corpus Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5b8e8c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = [word_tokenize(sentence) for sentence in corpus_all_comments]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96101b98",
   "metadata": {},
   "source": [
    "# W2V_Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed4c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  objectif : Prédire le contexte (les mots qui entourent) à partir d'un mot cible.\n",
    "#  Utilisation : Performant pour des corpus textuels où le sens des mots est relativement stable sur de courtes fenêtres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46607633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# # Modèle Word2Vec avec Skip-gram\n",
    "# Word2Vec_model_skipgram = Word2Vec(sentences=tokens, vector_size=100, window=5, sg=1, min_count=1, epochs=10)\n",
    "\n",
    "# # Sauvegarde du modèle\n",
    "# Word2Vec_model_skipgram.save(\"Word2Vec_Skipgram.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32b889-994e-4a23-b488-e243bebe2963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8126cc",
   "metadata": {},
   "source": [
    "# W2V_CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a940fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Prédire un mot cible à partir de son contexte (les mots qui l'entourent).\n",
    "#  Utilisation : Performant pour des corpus textuels où le sens des mots est relativement stable sur de courtes fenêtres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853c241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "Word2Vec_model_cbow = Word2Vec(tokens, vector_size=100, min_count=1, epochs=10, sg=0)\n",
    "\n",
    "Word2Vec_model_cbow.save(\"Word2Vec_CBOW.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ec978a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7b4fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_CBOW = Word2Vec.load(\"Word2Vec_CBOW.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = Word2Vec_CBOW.wv['film']  # get numpy vector of a 'film' word\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d13e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f743fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_comment_length(comment, max_words=100):\n",
    "    # Diviser le commentaire en mots\n",
    "    words = comment.split()\n",
    "    \n",
    "    # Limiter le nombre de mots\n",
    "    limited_words = words[:max_words]\n",
    "    \n",
    "    # Rejoindre les mots pour former un nouveau commentaire\n",
    "    limited_comment = ' '.join(limited_words)\n",
    "    \n",
    "    return limited_comment\n",
    "\n",
    "# Appliquer la fonction à chaque commentaire dans train_comments_list et dev_comments_list\n",
    "train_list = [limit_comment_length(comment) for comment in train_comments_list if comment]\n",
    "dev_list = [limit_comment_length(comment) for comment in dev_comments_list if comment]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c12df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(message):\n",
    "    # Diviser la chaîne en une liste de mots\n",
    "    words = message.split()\n",
    "    \n",
    "    # Compter le nombre de mots\n",
    "    num_words = len(words)\n",
    "    \n",
    "    return num_words\n",
    "\n",
    "# Exemple d'utilisation\n",
    "result = count_words(train_list[0])\n",
    "\n",
    "print(\"Nombre de mots dans le message :\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b1ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca401bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['nouvelle commentaire'] = train_list\n",
    "dev_data['nouvelle commentaire']   = dev_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dbe796",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['nouvelle commentaire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf237bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data sise : 665962"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd35b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['nouvelle commentaire']\n",
    "X_dev   = dev_data['nouvelle commentaire']\n",
    "\n",
    "# return the most 'len(X_train) = 665962' frequent words id\n",
    "tokenizer = Tokenizer(num_words=len(list(X_train.unique())))\n",
    "\n",
    "# entrainer le tokenizer\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "#mapping of words to ids for the entire text corpus\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Return a vector of ids of the most 'len(X_train) = 665962' in each comment\n",
    "X_train = tokenizer.texts_to_sequences(X_train) \n",
    "X_dev = tokenizer.texts_to_sequences(X_dev)\n",
    "\n",
    "# Ensure that all sequences in a list have the same length, if not add 0 at begining\n",
    "X_train = pad_sequences(X_train, maxlen=100)\n",
    "X_dev = pad_sequences(X_dev, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622d27df",
   "metadata": {},
   "source": [
    "# save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Sauvegarder le tokenizer\n",
    "with open('Cnn_tokenizer.pkl', 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb61c010",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda285a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f02038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Return a matix of vocab words embedding based using Word2Vec_CBOW \n",
    "def create_embedding_matrix(tokenizer, word2vec_model, embedding_dim):\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            vector = word2vec_model.wv[word]\n",
    "        except KeyError:\n",
    "            # Word not found in Word2Vec, keep the embedding as zeros\n",
    "            continue\n",
    "        embedding_matrix[i] = vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d30978",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = create_embedding_matrix(tokenizer, Word2Vec_CBOW, 100)\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfb258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['note'])\n",
    "y_train = y_train.astype(int)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb6e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev = np.array(dev_data['note'])\n",
    "y_dev = y_dev.astype(int)\n",
    "y_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eddd2e",
   "metadata": {},
   "source": [
    "# CNN + Word2Vec_CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0209acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f10608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv1D, GlobalMaxPool1D\n",
    "\n",
    "dropout_rate=0.25\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer to the model\n",
    "model.add(Embedding(\n",
    "    input_dim=vocab_size,         # Vocabulary size, total number of unique words in the data\n",
    "    output_dim=100,               # Dimension of the embedding space, each word represented by a vector of 100 dimensions\n",
    "    input_length=100,             # Length of each input sequence (comment), limited to 100 words\n",
    "    weights=[embedding_matrix],   # Initialize the embedding layer with pre-trained weights from embedding_matrix\n",
    "    trainable=True                # Allow the weights of the embedding layer to be trainable during model training\n",
    "))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu',kernel_regularizer=regularizers.l2(l=0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "# IMPROVEMENT\n",
    "# give equal class weights\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "# Convert class weights to a dictionary for use with Keras\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "#  sparse_categorical_crossentropy loss function used during training. For a classification task with integer labels (like 0, 1, 2),\n",
    "# for that i converted nmarks from 0.5,1...5 to 0,1...9\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Specify the file path where the best model will be saved\n",
    "filepath = 'CNN_Word2Vec_SkipGram_Best_One_improved.hdf5'\n",
    "\n",
    "# Create callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=filepath,\n",
    "    monitor='val_accuracy',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, early_stopping]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Train for more epochs\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_dev, y_dev),\n",
    "    epochs=20,  # Increase the number of epochs\n",
    "    batch_size=32,  # Experiment with different batch sizes\n",
    "    verbose=1,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict\n",
    ")\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(execution_time)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42c54fc",
   "metadata": {},
   "source": [
    "# save train and validation accuracy/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b01f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('Cnn_accuracy_plot.png')  # Save the plot as an image\n",
    "\n",
    "# Clear the figure for the next plot\n",
    "plt.figure()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.savefig('Cnn_loss_plot.png')  # Save the plot as an image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d54e44c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b293581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Charger le tokenizer depuis le fichier\n",
    "# with open('Cnn_tokenizer.pkl', 'rb') as tokenizer_file:\n",
    "#     loaded_tokenizer = pickle.load(tokenizer_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2648bb1b",
   "metadata": {},
   "source": [
    "# Test and saving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f56734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # model = load_model('MLP_Best_One.hdf5'),this command doesn't work for me, i had to change the saved model path\n",
    "\n",
    "# modelpath = \"C:/trained_Models/CNN_Word2Vec_SkipGram_Best_One_improved.hdf5\"\n",
    "\n",
    "# # Load the model with compile=False\n",
    "# CNN_Word2Vec_SkipGram_Best_One = load_model(modelpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395564e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb30e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data['commentaire'] = test_data['commentaire'].astype(str)\n",
    "# X_test = test_data['commentaire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ed145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# X_test = loaded_tokenizer.texts_to_sequences(X_test)\n",
    "# X_test = pad_sequences(X_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1f2764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = CNN_Word2Vec_SkipGram_Best_One.predict(X_test)\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a38eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bd3d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# argmax_predictions = np.argmax(predictions,axis =1)\n",
    "# argmax_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb02d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  generate the plateform test data format        \n",
    "# with open(\"CNN_Word2Vec_SkipGram_ID_Prediction.txt\", \"w\") as f:\n",
    "#     for i in range(len(test_data['review_id'])):\n",
    "#         prediction = (argmax_predictions[i] + 1) / 2\n",
    "#         line = f\"{test_data['review_id'].iloc[i]} {str(prediction).replace('.', ',')}\\n\"\n",
    "#         f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be97a9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97822e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
